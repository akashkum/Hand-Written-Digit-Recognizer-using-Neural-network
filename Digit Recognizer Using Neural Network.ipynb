{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 785), (28000, 784))"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y= train[\"label\"]\n",
    "train_X_unNormal = train.drop([\"label\"], axis = 1)\n",
    "test_X_unNormal  = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (28000, 784), (42000,))"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_unNormal.shape, test.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImage = train_X_unNormal.values\n",
    "trainImage = trainImage.reshape(42000, 28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee2f7b2e80>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = 46\n",
    "print(\"Label: \", train_Y[j])\n",
    "plt.imshow(trainImage[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X_unNormal/255.\n",
    "test_X = test_X_unNormal/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural network\n",
    "\n",
    "Now the Data is completely processed and ready to use as input for neural network\n",
    "To Build Neural Network following step should be followed\n",
    "1. Initializing parameters of an L layer neural Network ( I have to decide the architecture of network- Number of Hidden layer(L), hidden units per layers, Activation function for the layers) \n",
    "\n",
    "2. Loop for num_iterations:\n",
    "\n",
    "    a. Forward propagation   \n",
    "    b. Compute cost function    \n",
    "    c. Backward propagation\n",
    "    \n",
    "    d. Update parameters (using parameters, and grads from backprop) \n",
    "    \n",
    "4. Use trained parameters to predict labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 42000), (784, 28000), (1, 42000))"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Xf = train_X.T\n",
    "test_Xf = test_X.T\n",
    "train_Yf = train_Y.T\n",
    "train_Yf1 = train_Y.values\n",
    "train_Yf = train_Yf1.reshape(42000,1).T\n",
    "train_Xf.shape, test_Xf.shape, train_Yf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = train_Xf.shape[1]\n",
    "n_x = train_Xf.shape[0]\n",
    "m, n_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    L = len(layer_dims) - 1    # n_x is for input layer\n",
    "    parameters = {}\n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)] = np.random.randn(layer_dims[l+1], layer_dims[l])*0.01\n",
    "        parameters[\"b\"+str(l+1)] = np.zeros((layer_dims[l+1],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking Initializing function\n",
    "layer_dims = [4, 5, 6, 3,2]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "parameters[\"W1\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing helper functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) +b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "#array([[ 3.26295337, -1.23429987]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3.26295337, -1.23429987]]), (array([[ 1.62434536, -0.61175641],\n",
       "         [-0.52817175, -1.07296862],\n",
       "         [ 0.86540763, -2.3015387 ]]),\n",
       "  array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]),\n",
       "  array([[-0.24937038]])))"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_forward(A, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b , activation):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)  # linear cache =(A-prev, W, b)\n",
    "    activation_cache = Z\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A = np.maximum(0,Z)\n",
    "        \n",
    "    if activation == \"sigmoid\":\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "        \n",
    "    if activation == \"softmax\":\n",
    "        t = np.exp(Z)   #Z=> (l, m) -> t => (l,m)\n",
    "        t_sum_vertical = np.sum(t, axis=0,keepdims = True)\n",
    "        A = np.divide(t, t_sum_vertical)\n",
    "        \n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "    \n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Softmax: A = [[0.73105858 0.09112296]\n",
      " [0.26894142 0.90887704]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_prev = np.array([[1,0],[0,1],[0,1]])\n",
    "W = np.array([[2,1,.1],[1,3,.4]])\n",
    "b = np.array([[0]])\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"softmax\")\n",
    "print(\"With Softmax: A = \" + str(A))\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    L = len(parameters)//2\n",
    "    A_prev = X\n",
    "    caches = []\n",
    "    for l in range(1, L):\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        A_prev = A\n",
    "    A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], activation = \"softmax\")\n",
    "    caches.append(cache)\n",
    "    AL = A\n",
    "    return AL, parameters, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[1. 1.]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2}\n",
    "    \n",
    "AL, parameters, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[1. 1. 1. 1.]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(5,4)\n",
    "W1 = np.random.randn(4,5)\n",
    "b1 = np.random.randn(4,1)\n",
    "W2 = np.random.randn(3,4)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(1,3)\n",
    "b3 = np.random.randn(1,1)\n",
    "\n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "   \n",
    "AL, parameters, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Great! Now you have a full forward propagation that takes the input X and outputs a row vector $A^{[L]}$ containing your predictions. It also records all intermediate values in \"caches\". Using $A^{[L]}$, you can compute the cost of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(Y, K):\n",
    "    Y_onehot = np.zeros((K, Y.shape[1]))\n",
    "    for i in range(Y.shape[1]):\n",
    "        level = Y[0][i]\n",
    "        Y_onehot[level][i]=1\n",
    "    return Y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost function\n",
    "\n",
    "Now you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comput_cost_softmax(A, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -Y*np.log(A)\n",
    "    cost = np.sum(cost)\n",
    "    cost = cost/m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0601317681000455"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A= np.array([[.3,.4,.6,.7],\n",
    "            [.2,.4,.5,.6]])\n",
    "A = A.T\n",
    "Y = np.array([[1,0],\n",
    "             [0,1],\n",
    "             [0,0],\n",
    "             [0,0]])\n",
    "cost = comput_cost_softmax(A,Y)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backward propagation module\n",
    "\n",
    "<img src=\"BuildingBlocksBackprop1.jpeg\" style=\"width:450px;height:550px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    # cache have A_prev, W, b in it\n",
    "    A_prev, W, b = cache\n",
    "    m = dZ.shape[1]\n",
    "    \n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis =1, keepdims = True)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dZ = np.random.randn(1,2)\n",
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "linear_cache = (A, W, b)\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td> **dA_prev** </td>\n",
    "    <td > [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "        <td> **dW** </td>\n",
    "        <td > [[-0.10076895  1.40685096  1.64992505]] </td> \n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td> **db** </td>\n",
    "        <td> [[ 0.50629448]] </td> \n",
    "    </tr> \n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    A_prev , W, b = linear_cache\n",
    "    Z = activation_cache\n",
    "    if activation == \"sigmoid\":\n",
    "        sigma = 1/(1+np.exp(-Z))\n",
    "        dsigma_dZ = sigma*(1-sigma)\n",
    "        dZ = dA*dsigma_dZ\n",
    "    if activation == \"relu\":\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object    \n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "dA = np.random.randn(1,2)\n",
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "Z = np.random.randn(1,2)\n",
    "linear_cache = (A, W, b)\n",
    "activation_cache = Z\n",
    "linear_activation_cache = (linear_cache, activation_cache)\n",
    "dA_prev, dW, db = linear_activation_backward(dA, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dA, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td> \n",
    "\n",
    "  </tr>  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
    "  </tr>  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.05729622]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n",
    "\n",
    "**Expected output with relu:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "      <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    #caches is list of all layers cache = (linear_cache, activation_cache) \n",
    "    L = len(caches)\n",
    "    #print(\"Number of layers: \",L)\n",
    "    grads = {}\n",
    "    dAL = -(Y/AL) - ((1-Y)/(1-AL))\n",
    "    cache = caches[-1]\n",
    "\n",
    "    dZL = AL-Y\n",
    "    grads[\"dA\"+str(L-1)], grads[\"dW\"+str(L)], grads[\"db\"+str(L)] = linear_backward(dZL,cache[0] )\n",
    "    #grads[\"dA\"+str(L-1)], grads[\"dW\"+str(L)], grads[\"db\"+str(L)] = linear_activation_backward(dAL, cache, activation = \"sigmoid\")\n",
    "    for l in reversed(range(L-1)):\n",
    "        cache = caches[l]\n",
    "        grads[\"dA\"+str(l)], grads[\"dW\"+str(l+1)], grads[\"db\"+str(l+1)] = linear_activation_backward(grads[\"dA\"+str(l+1)], cache, activation = \"relu\")\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41642713 0.07927654 0.14011329 0.10664197]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05365169 0.01021384 0.01805193 0.01373955]]\n",
      "db1 = [[-0.22346593]\n",
      " [ 0.        ]\n",
      " [-0.02879093]]\n",
      "dA1 = [[-0.80745758 -0.44693186]\n",
      " [ 0.88640102  0.49062745]\n",
      " [-0.10403132 -0.05758186]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "AL = np.random.randn(1, 2)\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "A1 = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "Z1 = np.random.randn(3,2)\n",
    "linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
    "\n",
    "A2 = np.random.randn(3,2)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "Z2 = np.random.randn(1,2)\n",
    "linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
    "\n",
    "caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "\n",
    "grads = L_model_backward(AL, Y, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr>  \n",
    "    <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>\n",
    "\n",
    "\n",
    "### 6.4 - Update Parameters\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. \n",
    "\n",
    "**Exercise**: Implement `update_parameters()` to update your parameters using gradient descent.\n",
    "\n",
    "**Instructions**:\n",
    "Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters)//2\n",
    "    for l in range(1, L+1):\n",
    "        parameters[\"W\"+str(l)] = parameters[\"W\"+str(l)] - learning_rate*grads[\"dW\"+str(l)]\n",
    "        parameters[\"b\"+str(l)] = parameters[\"b\"+str(l)] - learning_rate*grads[\"db\"+str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2}\n",
    "np.random.seed(3)\n",
    "dW1 = np.random.randn(3,4)\n",
    "db1 = np.random.randn(3,1)\n",
    "dW2 = np.random.randn(1,3)\n",
    "db2 = np.random.randn(1,1)\n",
    "grads = {\"dW1\": dW1,\n",
    "         \"db1\": db1,\n",
    "         \"dW2\": dW2,\n",
    "         \"db2\": db2}\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr>   \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr>   \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate, num_iteration,hidden_layers, print_cost = False):\n",
    "    n_x = X.shape[0]\n",
    "    n_y = 10\n",
    "    layer_dims = [n_x]+ hidden_layers +[n_y]\n",
    "    #Initializeing parameters\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    Y1 = one_hot_encode(Y, 10)\n",
    "    \n",
    "    for i in range(num_iteration):\n",
    "        \n",
    "        AL, parameters, caches = L_model_forward(X, parameters)\n",
    "        cost = comput_cost_softmax(AL,Y1)\n",
    "        grads = L_model_backward(AL, Y1, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if (print_cost==True) and i%100==0:\n",
    "            print(\"cost after \",i, \" iterations:\",cost)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after  0  iterations: 2.3027304284136783\n",
      "cost after  100  iterations: 1.3345346891526544\n",
      "cost after  200  iterations: 0.6033945979447075\n",
      "cost after  300  iterations: 0.4529486085112411\n",
      "cost after  400  iterations: 0.3894617275495992\n",
      "cost after  500  iterations: 0.3542068629615557\n",
      "cost after  600  iterations: 0.3309131523209938\n",
      "cost after  700  iterations: 0.31369239356730527\n",
      "cost after  800  iterations: 0.3000386180974511\n",
      "cost after  900  iterations: 0.2887783035454055\n",
      "cost after  1000  iterations: 0.27922394969109676\n",
      "cost after  1100  iterations: 0.2709397372852224\n",
      "cost after  1200  iterations: 0.2636134886273709\n",
      "cost after  1300  iterations: 0.2569463100394571\n",
      "cost after  1400  iterations: 0.2508519473443292\n",
      "cost after  1500  iterations: 0.24529852820570924\n",
      "cost after  1600  iterations: 0.24020648874845965\n",
      "cost after  1700  iterations: 0.23545378131874795\n",
      "cost after  1800  iterations: 0.2310137314721877\n",
      "cost after  1900  iterations: 0.2268689231966764\n",
      "cost after  2000  iterations: 0.22294707374897674\n",
      "cost after  2100  iterations: 0.2192391720976082\n",
      "cost after  2200  iterations: 0.21570939769653966\n",
      "cost after  2300  iterations: 0.21232437337179372\n",
      "cost after  2400  iterations: 0.2090632592946556\n",
      "cost after  2500  iterations: 0.20592059438086574\n",
      "cost after  2600  iterations: 0.20288785893013747\n",
      "cost after  2700  iterations: 0.19994809603313635\n",
      "cost after  2800  iterations: 0.19709495499827565\n",
      "cost after  2900  iterations: 0.19432712593432566\n",
      "cost after  3000  iterations: 0.19165294816816142\n",
      "cost after  3100  iterations: 0.1890055467454161\n",
      "cost after  3200  iterations: 0.18641244440963844\n",
      "cost after  3300  iterations: 0.18388941846553356\n",
      "cost after  3400  iterations: 0.18144672242699816\n",
      "cost after  3500  iterations: 0.17905684303725794\n",
      "cost after  3600  iterations: 0.17673522606452563\n",
      "cost after  3700  iterations: 0.17447274855567\n",
      "cost after  3800  iterations: 0.17226267835529935\n",
      "cost after  3900  iterations: 0.17009806079114903\n",
      "cost after  4000  iterations: 0.16797835503593236\n",
      "cost after  4100  iterations: 0.16590448661617924\n",
      "cost after  4200  iterations: 0.16388221348467283\n",
      "cost after  4300  iterations: 0.16191075998186166\n",
      "cost after  4400  iterations: 0.15998402316818067\n",
      "cost after  4500  iterations: 0.15809491049568122\n",
      "cost after  4600  iterations: 0.15624240055734545\n",
      "cost after  4700  iterations: 0.15443180022232317\n",
      "cost after  4800  iterations: 0.15267240157322826\n",
      "cost after  4900  iterations: 0.1509441197476916\n",
      "cost after  5000  iterations: 0.14923189017255464\n",
      "cost after  5100  iterations: 0.14754308499116772\n",
      "cost after  5200  iterations: 0.14588374143222205\n",
      "cost after  5300  iterations: 0.1442468212246644\n",
      "cost after  5400  iterations: 0.14264057640168162\n",
      "cost after  5500  iterations: 0.14106511440618744\n",
      "cost after  5600  iterations: 0.1395239807594346\n",
      "cost after  5700  iterations: 0.13801393991482344\n",
      "cost after  5800  iterations: 0.13652825470782556\n",
      "cost after  5900  iterations: 0.13507343488052254\n",
      "cost after  6000  iterations: 0.13365931590121655\n",
      "cost after  6100  iterations: 0.132274517205867\n",
      "cost after  6200  iterations: 0.13091654984226922\n",
      "cost after  6300  iterations: 0.12958785975625775\n",
      "cost after  6400  iterations: 0.12827494393052116\n",
      "cost after  6500  iterations: 0.12699192929499076\n",
      "cost after  6600  iterations: 0.1257368555839036\n",
      "cost after  6700  iterations: 0.12450521347307471\n",
      "cost after  6800  iterations: 0.12329696471318939\n",
      "cost after  6900  iterations: 0.12211101056335529\n",
      "cost after  7000  iterations: 0.12094639764618846\n",
      "cost after  7100  iterations: 0.1198002289734906\n",
      "cost after  7200  iterations: 0.1186667939302569\n",
      "cost after  7300  iterations: 0.11755404085961302\n",
      "cost after  7400  iterations: 0.116464900981821\n",
      "cost after  7500  iterations: 0.11539073718341504\n",
      "cost after  7600  iterations: 0.11433530679405265\n",
      "cost after  7700  iterations: 0.11329809009972866\n",
      "cost after  7800  iterations: 0.11227955512696818\n",
      "cost after  7900  iterations: 0.11128028239821634\n",
      "cost after  8000  iterations: 0.11030110914000014\n",
      "cost after  8100  iterations: 0.10934204272214551\n",
      "cost after  8200  iterations: 0.10840127279893035\n",
      "cost after  8300  iterations: 0.10747727788419094\n",
      "cost after  8400  iterations: 0.10656611680325524\n",
      "cost after  8500  iterations: 0.10566908416564534\n",
      "cost after  8600  iterations: 0.10478422325991012\n",
      "cost after  8700  iterations: 0.10391019436806012\n",
      "cost after  8800  iterations: 0.10305130528827677\n",
      "cost after  8900  iterations: 0.10220567909745608\n",
      "cost after  9000  iterations: 0.10137455976509531\n",
      "cost after  9100  iterations: 0.1005572246421859\n",
      "cost after  9200  iterations: 0.09975160682113485\n",
      "cost after  9300  iterations: 0.09895465585555387\n",
      "cost after  9400  iterations: 0.09817198880249964\n",
      "cost after  9500  iterations: 0.09740212416637409\n",
      "cost after  9600  iterations: 0.09664353735079213\n",
      "cost after  9700  iterations: 0.09589693908811536\n",
      "cost after  9800  iterations: 0.09516136796129625\n",
      "cost after  9900  iterations: 0.09443366683077788\n"
     ]
    }
   ],
   "source": [
    "X = train_Xf\n",
    "Y = train_Yf\n",
    "learning_rate = 0.1\n",
    "num_iteration = 10000\n",
    "hidden_layers = [25]\n",
    "parameters = model(X, Y, learning_rate, num_iteration,hidden_layers, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    m = X.shape[1] \n",
    "    AL, parameters, caches = L_model_forward(X, parameters)\n",
    "    \n",
    "    K = AL.shape[0]\n",
    "    Y_predicted = np.zeros((1,m))\n",
    "    for i in range(m):\n",
    "        mx = max(AL[:,i])\n",
    "        for j in range(K):\n",
    "            if AL[j][i] == mx:\n",
    "                Y_predicted[0][i]=j\n",
    "                break\n",
    "    return  Y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = predict(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 97.42619047619047\n"
     ]
    }
   ],
   "source": [
    "accuracy = (np.sum(Y_predicted==Y)/m)*100\n",
    "print(\"Accuracy=\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = predict(test_Xf, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test= Y_test.astype(int)\n",
    "Y_subdf = pd.DataFrame(Y_test, index= ['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28000, 2), Index(['ImageId', 'Label'], dtype='object'))"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_sub = pd.read_csv('sample_submission - Copy.csv')   #TO read the columns from sample submission\n",
    "read_sub.shape, read_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_sub['Label'] = Y_subdf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImage = test_X_unNormal.values\n",
    "testImage = testImage.reshape(28000, 28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking manualy by showing image and predicted test label\n",
    "- Can be changed values of j between 0 to 27999 and can be varified predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee1ea59da0>"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = 9873\n",
    "print(\"Label: \", Y_test[0][j])\n",
    "plt.imshow(testImage[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Predicted Values in .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = read_sub.to_csv (r'C:\\Users\\akash\\Desktop\\Python Notebooks\\DigitRecognizer\\sample_submission.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
